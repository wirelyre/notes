# Scoring ordering questions

Some test questions look like this:

> Put the following values in ascending order:
> - The number of feet in a yard
> - The number of horns on a unicorn
> - The number of teaspoons in a tablespoon
> - The age of the universe, in years
> - The boiling point of water, in degrees Celcius

In order to find the correct answer, the test taker is supposed to roughly know
the number for each value, and then put the values in the correct order.  For
instance, one correct answer would be:

> - The number of horns on a unicorn (1)
> - The number of teaspoons in a tablespoon (3)
> - The number of feet in a yard (3)
> - The boiling point of water, in degrees Celcius (100)
> - The age of the universe, in years (roughly 14 billion)

Another common ordering question reads:

> Put the following events in chronological order.

In any case, it is easy to decide when the answer is exactly correct.  **But
how should we score the answer when it is only partially correct?**  What does
partially correct mean?


**TL;DR**: `{n←≠𝕩 ⋄ r←+´⥊𝕩∧○(≤⌜˜)↕n ⋄ (r-n)÷n×(n-1)÷2}`



## A wrong way to score

I've seen people give a single point for each value: 1 point if the value is in
the correct place, 0 points if it's not.  For example, this answer would give
3 points out of 5:

> teaspoons (3); unicorn (1); feet (3); water (100); universe (14 billion)

This is a bad way to score.  Firstly, it greatly penalizes a single misplaced
value: The answer ⟨**4**, 1, 2, 3, 5, 6⟩ yields only 2 points out of 6, even
though only 4 was misplaced.

Secondly, it values all swaps equally: The answers ⟨1, 2, **4**, **3**, 5, 6⟩
and ⟨**6**, 2, 3, 4, 5, **1**⟩ both yield 4 points, even though the latter is
the greatest single mistake possible.

All in all, a test taker with **partial information** is not rewarded properly
for demonstrating this information.



## A better way

The code here is in [BQN](https://mlochbaum.github.io/BQN/).

Instead of giving a point for each value, let's give a point for each pair of
values: 1 point if the pair is in the correct order, 0 points if it's not.

The example answer above, ⟨3, 1, 3, 100, 14e9⟩, would give 9 points out of 10.
We can see that there are 10 possible points because 5 objects have 10 possible
pairings.  The only mistake was swapping the first two values — hence 9 points.

Let's take all pairs of values in the answer (`≍⌜˜`) and determine if they're
correctly ordered (`≤`):

```bqn
≤⌜˜ ⟨3, 1, 3, 100, 14e9⟩

  ┌─           
  ╵ 1 0 1 1 1  
    1 1 1 1 1  
    1 0 1 1 1  
    0 0 0 1 1  
    0 0 0 0 1  
              ┘
```

We only care about the upper triangle of this matrix, because the lower
triangle represents pairs contrary to the ordering in the answer.  The upper
triangle is simply the pairs generated by a correctly ordered answer:

```bqn
≤⌜˜ ↕5

  ┌─           
  ╵ 1 1 1 1 1  
    0 1 1 1 1  
    0 0 1 1 1  
    0 0 0 1 1  
    0 0 0 0 1  
              ┘
```

We just need to `∧` these two matrices together, then take the sum of those
elements with `+´⥊`. Factoring `(≤⌜˜ 𝕩) ∧ (≤⌜˜ ↕5)` gives `𝕩 ∧○(≤⌜˜) ↕5`, so:

```bqn
Score ← {
  n←≠𝕩              # number of values
  r←+´⥊ 𝕩∧○(≤⌜˜)↕n  # raw score
}

Score ⟨3, 1, 3, 100, 14e9⟩

  14
```

Oops!  We also counted the diagonal elements.  Those are free points no matter
the answer, since each value is correctly ordered against itself, `x≤x`.
Subtracting the number of values fixes this, `r-n`.

Let's normalize the output to the range \[0, 1\].  What is the maximum raw score
of `n` values?  It's the `n-1` triangular number, `n×(n-1)÷2`.  (This is
correctly adjusted to not include the diagonal.)

Finally:

```bqn
Score ← {
  n←≠𝕩              # number of values
  r←+´⥊ 𝕩∧○(≤⌜˜)↕n  # raw score
  (r-n) ÷ n×(n-1)÷2
}

Score ⟨3, 1, 3, 100, 14e9⟩

  0.9
```



## Expected value

Suppose the test taker randomly guesses an answer.  What is the expected score?

We can imagine two equivalent random processes for two people, each trying to
produce a random answer.

Firstly, consider someone who doesn't know anything about the order.  They take
the first possible value and write it down in their answer.  Then they take the
second value and write it down either before or after that first value.  They
continue expanding their answer, putting each new value in a random spot in
their answer.

Secondly, consider someone who *does* know the order.  They can use the same
process, except they can start with the actual smallest value, and move up to
the largest value.

I think it's clear that these two processes both produce all permutations with
equal probability.  Therefore the expected score for each is the same.

The first process is one that a test taker can take.  The second process gives
a way to compute the expected score!  Assuming the values are **all distinct**,
each new value added will give 0 points if it is placed first (index 0) in the
answer (because it is the greatest value yet), or it will give *i* points if it
is placed at index *i*.

Intuitively, each new value increases the expected score by half of the
possible points.  If you work it out carefully, this is exactly true!

Using this method, the expected score of a random answer to an ordering
question with *n* distinct values is exactly **½**, for *n* ≥ 2.



## Strengths

This method is **straightforward** with **simple reasoning** for the
computation.

It produces results in a **well-defined range**, from 0 to 1.

It **rewards the knowledge** available to a particular test taker.  They can
shuffle around the values in their answer until it most accurately reflects
the information they have (even if their answer is ultimately inconsistent).

It **penalizes bigger mistakes**.  If a single value is displaced only a
little, the score is higher than if it is displaced very far.

I played around with it a little, and it seems to produce **intuitive and fair
results**.



## Weaknesses

This score is **difficult to compute by hand**.  It's very easy to miss a raw
point when scoring a single answer.  Without a computer, I wouldn't trust
myself to score even one question with four values, let alone tens of questions
or questions with more values.

The **expected score** is ½ regardless of the number of values to be ordered.
By contrast, the expected score of a multiple-choice question with *n* answers
is only 1÷*n*.  Depending on the test and question, it might be better to curve
the score (for example, by squaring it, so that the final raw points are the
most valuable).

If **multiple options have the same value** and can be reordered, the expected
score of a random answer rises.  This can be curved away, or avoided entirely
when writing the question.

Consider a question like this: *In what year was the sack of Constantinople?
(Answers within 5 years of the correct answer recieve full points.)*  The
scoring is **lenient** because multiple answers are correct.  It seems like it
should be possible to make the scoring of an ordering question lenient, but
it's not clear to me how to do this.  (In case the tolerance for values is
related to the value itself, it might be possible to apply a transformation
first; for example, by taking the logarithm of the raw values.)

The **runtime** of this algorithm is O(*n*²) where *n* is the number of values.
There might be a better way to compute the same score.  This is actually deeply
related to the way we model the information a test taker has:  We assume that
they know facts like "this comes before that" and nothing else. Assuming a
different model of knowledge, *e.g.* the exponential order of values, might
result in an entirely different scoring system.

— 2023 February 19
